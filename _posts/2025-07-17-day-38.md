---
layout: post
title: "Day 38 â€“ Day 38 of ceamls"
date: 2025-07-17
author: Ignatius Nwankwo
permalink: /day38.html
tags: ["Loss","Optim" ,"Class Weights", "Early stopping", "Reduce Learning Rate on Plateau", "adamW", "Dropout", "callbacks", "verbose", "Restore Best Weights", "Training Weights"]

what_i_learned: |  
  we had a writing workshop today. The part that interested me the most was about sportds betting and how much money it makes annually in the state of maryland ever since it was legalized. I didnt even know it was illegal before. My results for efficientnetb4 using the latest hyperparameters was very poor. I began training b5 with my optuna hyperparameters on this new dataset, and it should hopefully be done by the end of the day or tomorrow.

blockers: |
  I had to re-migrate the new dataset into my work pc because i realized that training models is easier there unlike my laptop. I also noticed that I am not the most organized and that my coding knowledge is pretty limited but I try to learn from each mistake. Partly due to the personality workshop done on Tuesday, as an ISTJ, I noticed that I tend to have perfectionistic habits that stop me from learning new things and I'm trying to work on that as well.

reflection: |
  Upon further inspection of my codebase, I found out that I was never actually training Optuna on b5, but was actually running it on b4 the entire time. I was changing the wrong variable, one that was never reused throughout the entire code. Though its not a massive failure, it does change things. I think this model will be the last that I am training. In the meantime, I will reorganize my work and spend the remainder of my time putting the research paper together. I will also research more on batch normalization and develop the diagram for my current models to add to the research paper.
---
