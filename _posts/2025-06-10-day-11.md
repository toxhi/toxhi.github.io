---
layout: post
title: "Day 11 â€“ Day 11 of ceamls"
date: 2025-06-10
author: Ignatius Nwankwo
permalink: /day11.html
tags: ["Training Loss", "Validation Loss","Confusion Matrix"]

what_i_learned: |
   Ive continued my research on efficient net and densenet and I refreshed my memory on depthwise-separable convolution. I later retrained our dataset using densenet121 as opposed to mobilenetv2 as the base model to see if I would get an improvement in the confusion matrix. Though I didn't get much improvement, this information is still useful and hopefully combining more models will result in a better outcome. After lunch I tested EfficientNetB0, which yielded poor results, likely due to an error when running the code.



blockers: |
   The concept of general and depthwise separable convolution was tricky but particular youtube videos helped me understand the importance of the latter in order to reduce the number of computations and parameters. Upon reimporting the data set during the efficientnetB0 trial I had file path errors which were resolved by repasting the path but for some reason the images were imported as 3 classes instead of 1, which messed up the processing of the result. Subsequently, re-retrieving the files created more classes, which I did not want. I solved this by duplicating the first notebook I worked with and replacing the model in that one with mobilenet and starting afresh. Even after managing to retrieve only 2 classes of data, the model failed to give me a reliable result in the confusion matrix.



reflection: |
   I realized that rushing learning and overwhelming yourself will quickly lead to burnout, and that it's better to digest information bits and pieces at a time. This process enabled me to better understand the architecture of the models. Practicing using the models and seeing them in action helps to further solidify my understanding. I hope to share this with my grad mentor and learn how to concatenate all three models.


   



---
