---
layout: post
title: "Day 43 â€“ Day 43 of ceamls"
date: 2025-07-24
author: Ignatius Nwankwo
permalink: /day43.html
tags: [""]

what_i_learned: |  
  Yesterday I gave one of my lab mates one of my notebooks to run on their own computer. The notebook contained b4 wihtout hyperparameter tuning and used the old dataset. She combined it with another notebook i sent her that was trained using b5 using hyperparameters provided by running Optuna on b4. This morning, the notebook provided abysmal results, so I sent her a better notebook that was also b4 without hyperparameter tuning and used the old dataset. Our grad mentor came today and gave feedback on our methodology as well as helped us make some finishing changes to our elevator pitch. I spend the later half of my day updating the metrics table for efficientnet B0-B4 on the research paper.
  
blockers: |
  I took the time to look through all of the notebooks for all of the efficienet models that I trained ranging from B0-b4 and noted their accuracies, f1-score, precision and recall. It was initially very overwhelming, cause I did a LOT of training, and it was initally kinda hard to obtain this data because of naming convention and organizational limitations, as well as not knowing how to output the data properly in the code at the time, but I bookmarked each google tab to help me keep everything organized, as well as starred the notebooks that stood out to me the most. as well as observed the validation accuracy in the model.fit() output to obtain the final validation accuracy from there. I had issues deploying my model onto huggingface and one of my labmates is helping me with that.
  
  
reflection: |
  By looking through my models, I was able to keep track of how the predictions imporved over time, as well as manually deduce the overall precision and recall by averaging the precision and recall values for drowsy '0' and non drowsy '1' predictions respectively. I was also able to understand how it influences our F1-score, however one of my models ran into a problem and did not categorize the images properly, so the F1 score ended up not making sense. Throughout the training of the models, I also noticed that our team alternated between two different splitting ratios. Some models used a ratio of (.75[train],.10[test],.15[val]) and others, a ratio of (.8[train],.15[test],.05[val]) splitting ratio. I have yet to understand which one gave better results though. We later plan on deploying our codebase on huggingface, which will allow for real-time detection of drowsy and non drowsy states through video feed. I'll also go research the differences in data when it came to utilizing different optimizers such as Adam and AdamW as well as learning rates. I suppose this is where my Optuna trials will come in handy.

---
  
